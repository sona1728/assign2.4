1.)
*HADOOP IN LAYMAN'S TERM:
- In simple terms, Hadoop is a collection of open source programs/procedures relating to Big Data analysis. Being open source, it is freely available for use, reuse and modification (with some restrictions) for anyone who is interested in it. Big Data scientists call Hadoop the ‘backbone’ of their operations. In fact, Hadoop certification has become the next stepping stone for experienced software professionals who are starting to feel stagnated in their current stream.

- Hadoop comprises mainly of 4 modules namely, DFS, MapReduce, YARN and Hadoop Common. Each of these module has a specific task assigned to it for facilitating Big Data Analytics.

1.HADOOP COMMON:
-  Hadoop Common provides Java-based user tools that are to be used for accessing and retrieving data stored in a Hadoop file system.
 
2.DISTRIBUTED FILE SYSTEM (DFS):
- In Hadoop, Distributed File System is what enables data to be stored in a form that can be easily accessed and retrieved. The data will be stored across several interconnected devices which can be reached for using MapReduce.
 
3.YARN:
- The fourth and final module, YARN manages the system resources when the analytics are being conducted on the data stored in linked devices.
 
4.MAPREDUCE:
- MapReduce basically does two primary functions: it reads data from databases and puts them into a format that is suitable for Big Data Analytics. Further, it breaks down the data into meaningful information that can be used for interpretation.
 
 
 2.)
*COMPONENTS OF HADOOP FRAMEWORK:

The Hadoop Ecosystem comprises of 4 core components –

#Hadoop Common:

- Apache Foundation has pre-defined set of utilities and libraries that can be used by other modules within the Hadoop ecosystem. For example, if HBase and Hive want to access HDFS they need to make of Java archives (JAR files) that are stored in Hadoop Common.

#Hadoop Distributed File System (HDFS):

- The default big data storage layer for Apache Hadoop is HDFS. HDFS is the “Secret Sauce” of Apache Hadoop components as users can dump huge datasets into HDFS and the data will sit there nicely until the user wants to leverage it for analysis. HDFS component creates several replicas of the data block to be distributed across different clusters for reliable and quick data access. HDFS comprises of 3 important components-NameNode, DataNode and Secondary NameNode. HDFS operates on a Master-Slave architecture model where the NameNode acts as the master node for keeping a track of the storage cluster and the DataNode acts as a slave node summing up to the various systems within a Hadoop cluster.

#Map Reduce:

- MapReduce is a Java-based system created by Google where the actual data from the HDFS store gets processed efficiently. MapReduce breaks down a big data processing job into smaller tasks. MapReduce is responsible for the analysing large datasets in parallel before reducing it to find the results.

#YARN:

- YARN forms an integral part of Hadoop 2.0.YARN is great enabler for dynamic resource utilization on Hadoop framework as users can run various Hadoop applications without having to bother about increasing workloads.


3.)
*REASONS TO LEARN BIG DATA TECHNOLOGY:
- The amount of data out there is astounding — and data sets are large enough nowadays that traditional data processing technologies such as relational database management systems (RDBMS) or data centers are no longer enough.

*SOME REASONS ARE:

# No signs of slowing down:
- What’s fueled the meteoric rise of larger data sets? More people have access to mobile devices that sense and acquire information through the likes of cameras, microphones, etc. Consider this, since 2012, about 2.5 exabytes of data are created daily! That said, the Big Data revolution will continue to grow.

#  Everyone uses Big Data:
- Think Big Data is limited to IT circles? Think again. Big Data is everywhere, from politics to health care to even sports. Big Data analysis played a crucial role in President Obama’s successful reelection in 2012. Healthcare organizations use it to provide more personalized prescriptions, predictive analysis, and many other services. And sports-wise, more teams are using Big Data analysis to scout for athletes who best fit their needs.

# Information Managers in Demand:
- Someone has to be able to implement, run, and manage the software used to analyze Big Data. So, in conjunction with the rise of Big Data, the demand for information management specialists has increased. 

# Demand for Big Data skills is extremely high, and being able to prove your expertise is of essence.

# 64% of IT hiring managers rate skilled big data knowledge as having extremely high or high value when rating expertise of candidates; this is based on a survey by CompTIA.

# According to Forbes, the median advertised salary for professionals with Big Data expertise is $124,000 a year.

# IBM, Cisco, and Oracle together advertised 26,488 open positions that required Big Data expertise in the last twelve months.
